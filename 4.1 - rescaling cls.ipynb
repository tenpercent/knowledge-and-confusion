{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "id": "bGqvwBbDtz0t",
    "outputId": "a9a3392a-89ef-4141-a226-464f73afa8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: boto3 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (1.17.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (2.20.1)\n",
      "Requirement already satisfied: sacremoses in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (2018.1.18)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers as trf\n",
    "import torch as pt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KOv1_C81t6WA",
    "outputId": "f86e5f25-dd5b-43eb-91b8-f2e3952268e3"
   },
   "outputs": [],
   "source": [
    "bert = trf.BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = trf.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6Y0aPY8ImS7"
   },
   "outputs": [],
   "source": [
    "def norm_of(word):\n",
    "  with pt.no_grad():\n",
    "    return bert.embeddings.word_embeddings.weight[tokenizer.convert_tokens_to_ids([word])[0]].norm().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCjVAb4UJZxP"
   },
   "outputs": [],
   "source": [
    "def index_of(word):\n",
    "  return tokenizer.convert_tokens_to_ids([word])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(tk_ids, model=bert, tokenizer=tokenizer):\n",
    "  print(f'input tokens: {tk_ids}')\n",
    "  print(f'input sentence: {tokenizer.decode(tk_ids)}')\n",
    "  with pt.no_grad():\n",
    "    outs, _ = model(pt.tensor(tk_ids).unsqueeze(0))\n",
    "  outs.detach()\n",
    "  dots = pt.matmul(outs, model.embeddings.word_embeddings.weight.T)\n",
    "  softmaxes = dots.softmax(dim=-1)\n",
    "  predictions = softmaxes.argmax(dim=-1).squeeze()\n",
    "  print(f'predicted tokens: {tokenizer.convert_ids_to_tokens(predictions)}')\n",
    "  decoded = tokenizer.decode(predictions)\n",
    "  print(f'decoded sentence: {decoded}')\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2414, 2003, 1996, 3007, 1997, 2307, 3725, 1012, 102]\n",
      "input sentence: [CLS] london is the capital of great britain. [SEP]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', 'capital', '[CLS]', 'ned', '[CLS]', '##ann', '##vis']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] capital [CLS] ned [CLS]annvis\n",
      "input tokens: tensor([  101,   101,   101,   101,  3007,   101, 12311,   101, 11639, 11365])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] capital [CLS] ned [CLS]annvis\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/ipy/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'london is the capital of great britain.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsCsukYBI3sX"
   },
   "outputs": [],
   "source": [
    "the_factor = norm_of('[CLS]') / norm_of('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zR-c4lvAJEgL"
   },
   "outputs": [],
   "source": [
    "cls_factor = 1 / the_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EqQCToOT5fMj",
    "outputId": "dafca050-4eef-4474-c3b0-99021460400b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOPff9Sv4yXH"
   },
   "outputs": [],
   "source": [
    "bert.embeddings.word_embeddings.weight[index_of('the')] *= the_factor\n",
    "bert.embeddings.word_embeddings.weight[index_of('[CLS]')] *= cls_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "K3f88j6-JroL",
    "outputId": "bf830ffa-2584-435c-f836-46e7c736f98d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.030848979949951"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_of('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VTa5mejMJtAT",
    "outputId": "2f555d29-07f1-4f05-869e-3ef54fb12307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216485381126404"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_of('[CLS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfUG_rHxuOa_"
   },
   "outputs": [],
   "source": [
    "def reconstruct(tk_ids, model=bert, tokenizer=tokenizer):\n",
    "  print(f'input tokens: {tk_ids}')\n",
    "  print(f'input sentence: {tokenizer.decode(tk_ids)}')\n",
    "  with pt.no_grad():\n",
    "    outs, _ = model(pt.tensor(tk_ids).unsqueeze(0))\n",
    "  outs.detach()\n",
    "  dots = pt.matmul(outs, model.embeddings.word_embeddings.weight.T)\n",
    "  softmaxes = dots.softmax(dim=-1)\n",
    "  predictions = softmaxes.argmax(dim=-1).squeeze()\n",
    "  print(f'predicted tokens: {tokenizer.convert_ids_to_tokens(predictions)}')\n",
    "  decoded = tokenizer.decode(predictions)\n",
    "  print(f'decoded sentence: {decoded}')\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "dPaEDXPHyZVd",
    "outputId": "6db553ad-c3b0-4956-9276-4d923f3129a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2414, 2003, 1996, 3007, 1997, 2307, 3725, 1012, 102]\n",
      "input sentence: [CLS] london is the capital of great britain. [SEP]\n",
      "predicted tokens: ['##city', 'london', 'the', 'the', 'capital', 'the', 'ned', 'britain', '##ann', '##vis']\n",
      "decoded sentence: ##city london the the capital the ned britainannvis\n",
      "input tokens: tensor([12972,  2414,  1996,  1996,  3007,  1996, 12311,  3725, 11639, 11365])\n",
      "input sentence: ##city london the the capital the ned britainannvis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'london is the capital of great britain.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "uLsCrREryr1t",
    "outputId": "3f84add2-f312-4d02-b668-5dbf7bd79936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2300, 3774, 1997, 9732, 1998, 7722, 1012, 102]\n",
      "input sentence: [CLS] water consists of hydrogen and oxygen. [SEP]\n",
      "predicted tokens: ['##lus', 'the', 'the', 'the', 'hydrogen', 'the', 'oxygen', '##ther', '##ther']\n",
      "decoded sentence: ##lus the the the hydrogen the oxygentherther\n",
      "input tokens: tensor([ 7393,  1996,  1996,  1996,  9732,  1996,  7722, 12399, 12399])\n",
      "input sentence: ##lus the the the hydrogen the oxygentherther\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'water consists of hydrogen and oxygen.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "VzcNQ2zvy4cE",
    "outputId": "99571ec3-8977-44f5-d966-8d6c611ef6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2899, 2003, 1996, 3007, 1997, 2142, 2163, 1012, 102]\n",
      "input sentence: [CLS] washington is the capital of united states. [SEP]\n",
      "predicted tokens: ['##cas', 'washington', 'the', 'the', 'capitals', 'the', 'usa', 'puget', '##vey', '##vna']\n",
      "decoded sentence: ##cas washington the the capitals the usa pugetveyvna\n",
      "input tokens: tensor([15671,  2899,  1996,  1996, 15433,  1996,  3915, 27879, 12417, 29207])\n",
      "input sentence: ##cas washington the the capitals the usa pugetveyvna\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'washington is the capital of united states.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "id": "2sKdJPZky_7c",
    "outputId": "85b71cd9-603a-43fa-cf0f-61e5408e882c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 1996, 9598, 1997, 2710, 2003, 1996, 3010, 7922, 1012, 102]\n",
      "input sentence: [CLS] the currency of canada is the canadian dollar. [SEP]\n",
      "predicted tokens: ['##lus', '##chet', 'currency', 'the', 'canada', 'the', '##nated', 'canadian', 'dollar', '##ann', '##ust']\n",
      "decoded sentence: ##luschet currency the canada thenated canadian dollarannust\n",
      "input tokens: tensor([ 7393, 20318,  9598,  1996,  2710,  1996, 23854,  3010,  7922, 11639,\n",
      "        19966])\n",
      "input sentence: ##luschet currency the canada thenated canadian dollarannust\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the currency of canada is the canadian dollar.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWyEq_V45tEj"
   },
   "outputs": [],
   "source": [
    "bert.embeddings.word_embeddings.weight[103] /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "id": "PFS00KZ55weB",
    "outputId": "7510638b-2566-42ae-8a17-022561098dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 1996, 9598, 1997, 2710, 2003, 1996, 3010, 7922, 1012, 102]\n",
      "input sentence: [CLS] the currency of canada is the canadian dollar. [SEP]\n",
      "predicted tokens: ['##lus', '##chet', 'currency', 'the', 'canada', 'the', '##nated', 'canadian', 'dollar', '##ann', '##ust']\n",
      "decoded sentence: ##luschet currency the canada thenated canadian dollarannust\n",
      "input tokens: tensor([ 7393, 20318,  9598,  1996,  2710,  1996, 23854,  3010,  7922, 11639,\n",
      "        19966])\n",
      "input sentence: ##luschet currency the canada thenated canadian dollarannust\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n",
      "input tokens: tensor([1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996])\n",
      "input sentence: the the the the the the the the the the the\n",
      "predicted tokens: ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "decoded sentence: the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the currency of canada is the canadian dollar.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT-scaled reconstruction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
