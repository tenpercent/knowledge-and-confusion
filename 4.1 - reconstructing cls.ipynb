{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "id": "bGqvwBbDtz0t",
    "outputId": "27ffcd11-70d4-46fa-c1dc-6f08f277e3b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (1.17.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (4.45.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: sacremoses in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: boto3 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (2.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from requests->transformers) (2018.1.18)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/miniconda3/envs/ipy/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers as trf\n",
    "import torch as pt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOv1_C81t6WA"
   },
   "outputs": [],
   "source": [
    "bert = trf.BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = trf.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfUG_rHxuOa_"
   },
   "outputs": [],
   "source": [
    "def reconstruct(tk_ids, model=bert, tokenizer=tokenizer):\n",
    "  print(f'input tokens: {tk_ids}')\n",
    "  print(f'input sentence: {tokenizer.decode(tk_ids)}')\n",
    "  with pt.no_grad():\n",
    "    outs, _ = model(pt.tensor(tk_ids).unsqueeze(0))\n",
    "  outs.detach()\n",
    "  dots = pt.matmul(outs, model.embeddings.word_embeddings.weight.T)\n",
    "  softmaxes = dots.softmax(dim=-1)\n",
    "  predictions = softmaxes.argmax(dim=-1).squeeze()\n",
    "  print(f'predicted tokens: {tokenizer.convert_ids_to_tokens(predictions)}')\n",
    "  decoded = tokenizer.decode(predictions)\n",
    "  print(f'decoded sentence: {decoded}')\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "dPaEDXPHyZVd",
    "outputId": "f522a26d-2e78-4e8e-9832-873aa338a86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2414, 2003, 1996, 3007, 1997, 1996, 2142, 2983, 1012, 102]\n",
      "input sentence: [CLS] london is the capital of the united kingdom. [SEP]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', 'capital', '[CLS]', '[CLS]', 'uk', 'uk', '##ann', '##urn']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] capital [CLS] [CLS] uk ukannurn\n",
      "input tokens: tensor([  101,   101,   101,   101,  3007,   101,   101,  2866,  2866, 11639,\n",
      "        14287])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] capital [CLS] [CLS] uk ukannurn\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/ipy/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'london is the capital of the united kingdom.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "uLsCrREryr1t",
    "outputId": "1233b7c2-6fa3-438a-b676-8ac3643a2d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2300, 3774, 1997, 9732, 1998, 7722, 1012, 102]\n",
      "input sentence: [CLS] water consists of hydrogen and oxygen. [SEP]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '##rona', 'hydrogen', '[CLS]', 'oxygen', '##ther', '##ther']\n",
      "decoded sentence: [CLS] [CLS] [CLS]rona hydrogen [CLS] oxygentherther\n",
      "input tokens: tensor([  101,   101,   101, 26788,  9732,   101,  7722, 12399, 12399])\n",
      "input sentence: [CLS] [CLS] [CLS]rona hydrogen [CLS] oxygentherther\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'water consists of hydrogen and oxygen.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "VzcNQ2zvy4cE",
    "outputId": "c6cf7d3c-b88d-4cb3-f6e8-5133bfe670d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 2899, 2003, 1996, 3007, 1997, 2142, 2163, 1012, 102]\n",
      "input sentence: [CLS] washington is the capital of united states. [SEP]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', 'capitals', '[CLS]', '[CLS]', '[CLS]', '##vey', '##vna']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] capitals [CLS] [CLS] [CLS]veyvna\n",
      "input tokens: tensor([  101,   101,   101,   101, 15433,   101,   101,   101, 12417, 29207])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] capitals [CLS] [CLS] [CLS]veyvna\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'washington is the capital of united states.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "id": "2sKdJPZky_7c",
    "outputId": "4e8f9aab-d0e5-42ad-be9e-d1f6d1f23729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: [101, 1996, 9598, 1997, 2710, 2003, 1996, 3010, 7922, 1012, 102]\n",
      "input sentence: [CLS] the currency of canada is the canadian dollar. [SEP]\n",
      "predicted tokens: ['[CLS]', '##ider', 'currency', 'exercised', 'canada', '[CLS]', '##nated', 'canadian', 'dollar', '##ann', '##ust']\n",
      "decoded sentence: [CLS]ider currency exercised canada [CLS]nated canadian dollarannust\n",
      "input tokens: tensor([  101, 18688,  9598, 17747,  2710,   101, 23854,  3010,  7922, 11639,\n",
      "        19966])\n",
      "input sentence: [CLS]ider currency exercised canada [CLS]nated canadian dollarannust\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "input tokens: tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101])\n",
      "input sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "predicted tokens: ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
      "decoded sentence: [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the currency of canada is the canadian dollar.'\n",
    "tks = tokenizer.encode(sentence)\n",
    "for _ in range(10):\n",
    "  tks = reconstruct(tks)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT-orig reconstruction-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
